{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import os\n",
    "import torch.utils.data as D\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def construct_list(filepath):\n",
    "    text = open(filepath, \"r\",encoding=\"utf-8\").readlines()\n",
    "    list_of_tuples = []\n",
    "    for sentence_and_label in text:\n",
    "        temp = sentence_and_label.split(\"\\t\")\n",
    "        temp_tuple = (temp[0], int(temp[1][0]))\n",
    "        list_of_tuples.append(temp_tuple)\n",
    "    return list_of_tuples\n",
    "print(construct_list(\"./corpus/test.txt\"))\n",
    "list_of_stop_word = open(\"./corpus/stop_words.txt\", \"r\", encoding=\"utf-8\").readlines()\n",
    "list_of_stop_word = [line.strip(\"\\n\") for line in list_of_stop_word]\n",
    "print(list_of_stop_word)\n",
    "\n",
    "def list_of_sentence_splitted(filepath):\n",
    "    sentence_and_label_list = construct_list(filepath)\n",
    "    list_of_sentence_splitted = []\n",
    "    for sentence_and_label in sentence_and_label_list:\n",
    "        sentence = sentence_and_label[0]\n",
    "        sentence_splitted = jieba.lcut(sentence)\n",
    "        temp_list = []\n",
    "        for word in sentence_splitted:\n",
    "            if word in list_of_stop_word:\n",
    "                continue\n",
    "            else:\n",
    "                temp_list.append(word)\n",
    "        list_of_sentence_splitted.append(temp_list)\n",
    "    return list_of_sentence_splitted\n",
    "print(list_of_sentence_splitted(\"./corpus/test.txt\"))\n",
    "\n",
    "def construct_dict(list_of_sentences_splitted):\n",
    "    wordlist = []\n",
    "    for sentence in list_of_sentences_splitted:\n",
    "        for word in sentence:\n",
    "            wordlist.append(word)\n",
    "    wordset = set(wordlist)\n",
    "    wordlist = list(wordset)\n",
    "    worddict_word_to_index = {}\n",
    "    worddict_index_to_word = {}\n",
    "    for i in range(0, len(wordlist)):\n",
    "        worddict_word_to_index[wordlist[i]] = i\n",
    "        worddict_index_to_word[i] = wordlist[i]\n",
    "    worddict_word_to_index['<PAD>'] = -1\n",
    "    worddict_index_to_word[-1] = '<PAD>'\n",
    "    return worddict_word_to_index, worddict_index_to_word\n",
    "w2i, i2w = construct_dict(list_of_sentence_splitted(\"./corpus/train.txt\"))\n",
    "print(w2i)\n",
    "print(i2w)\n",
    "\n",
    "def compute_feature_and_label(filepath):\n",
    "    max_length = 0\n",
    "    list_of_list = list_of_sentence_splitted(filepath)\n",
    "    for sentence in list_of_list:\n",
    "        length = len(sentence)\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "    for sentence in list_of_list:\n",
    "        for i in range(0, max_length - len(sentence)):\n",
    "            sentence.append('<PAD>')\n",
    "\n",
    "    feature = []\n",
    "    w2i, i2w = construct_dict(list_of_list)\n",
    "    for sentence in list_of_list:\n",
    "        index_list = []\n",
    "        for word in sentence:\n",
    "            index = w2i[word]\n",
    "            index_list.append(index)\n",
    "        feature.append(index_list)\n",
    "    tensor_of_feature = torch.IntTensor(feature)\n",
    "    label = [sentence[1] for sentence in construct_list(filepath)]\n",
    "    tensor_of_label = torch.IntTensor(label)\n",
    "    return tensor_of_feature, tensor_of_label\n",
    "feature, label = compute_feature_and_label(\"./corpus/test.txt\")\n",
    "print(feature.shape)\n",
    "print(feature)\n",
    "print(label.shape)\n",
    "print(label)\n",
    "train_feature, train_label = compute_feature_and_label(\"./corpus/train.txt\")\n",
    "test_feature, test_label = compute_feature_and_label(\"./corpus/test.txt\")\n",
    "train_dataset = D.TensorDataset(train_feature, train_label)\n",
    "test_dataset = D.TensorDataset(test_feature, test_label)\n",
    "train_dataloader = D.DataLoader(train_dataset, batch_size=32)\n",
    "test_dataloader = D.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d, self).__init__()\n",
    "    def forward(self, x):\n",
    "         # x shape: (batch_size, channel, seq_len)\n",
    "        return F.max_pool1d(x, kernel_size=x.shape[2]) # shape: (batch_size, channel, 1)\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, kernel_sizes, num_channels):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=-1)  # embedding之后的shape: torch.Size([200, 8, 300])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_channels), 4)\n",
    "        # 时序最大池化层没有权重，所以可以共用一个实例\n",
    "        self.pool = GlobalMaxPool1d()\n",
    "        self.convs = nn.ModuleList()  # 创建多个一维卷积层\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels = embedding_dim, \n",
    "                                        out_channels = c, \n",
    "                                        kernel_size = k))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        embeds = embeds.permute(0, 2, 1)\n",
    "        # 对于每个一维卷积层，在时序最大池化后会得到一个形状为(批量大小, 通道大小, 1)的\n",
    "        # Tensor。使用flatten函数去掉最后一维，然后在通道维上连结\n",
    "        encoding = torch.cat([self.pool(F.relu(conv(embeds))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        # 应用丢弃法后使用全连接层得到输出\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs\n",
    "\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_iter):\n",
    "            X, y = batch.text, batch.label\n",
    "            X = X.permute(1, 0)\n",
    "            y.data.sub_(1)  #X转置 y下标从0开始\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: # 自定义的模型\n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "for data_batch, target_batch in train_dataloader:\n",
    "    print(data_batch)\n",
    "    print(target_batch)\n",
    "def train(train_iter, test_iter, net, loss, optimizer, num_epochs):\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for batch, label in train_iter:\n",
    "            X, y = batch, label\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print(\n",
    "            'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "            % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n,\n",
    "               test_acc, time.time() - start))\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "net = TextCNN(len(w2i), 8, kernel_sizes=[2, 4], num_channels=[4, 5])\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_dataloader, test_dataloader, net, loss, optimizer, num_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
